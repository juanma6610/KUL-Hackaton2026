{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1e39e3d",
   "metadata": {},
   "source": [
    "XGBoost implementation of the half life regression baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "905252bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\juanm\\miniconda3\\envs\\kulhack\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from scipy.stats import spearmanr\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "from sys import intern\n",
    "import optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "541427d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# various constraints on parameters and outputs\n",
    "MIN_HALF_LIFE = 15.0 / (24 * 60)    # 15 minutes\n",
    "MAX_HALF_LIFE = 274.                # 9 months\n",
    "LN2 = math.log(2.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b52af8fb",
   "metadata": {},
   "source": [
    "## MCM Portion of the tree (May need revisting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0fb9db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MultiscaleContextModel:\n",
    "    def __init__(self, mu=0.01, nu=1.05, xi=0.9, N=100, eps_r=9.0):\n",
    "        \"\"\"\n",
    "        Initializes the MCM memory state for a single item.\n",
    "        \n",
    "        Parameters:\n",
    "        - mu, nu: Control the distribution of time scales (decay rates).\n",
    "        - xi: Controls the weighting of different time scales.\n",
    "        - N: Number of context pools (integrators).\n",
    "        - eps_r: The boost given to successful retrieval (usually > 1).\n",
    "        \"\"\"\n",
    "        self.N = N\n",
    "        self.eps_r = eps_r\n",
    "        \n",
    "        # 1. Initialize Time Scales (tau) and Weights (gamma)\n",
    "        indices = np.arange(1, N + 1)\n",
    "        self.tau = mu * (nu ** indices)\n",
    "        self.gamma = xi ** indices\n",
    "        self.gamma = self.gamma / np.sum(self.gamma) # Normalize to sum to 1\n",
    "        \n",
    "        # Precompute cumulative sums of gamma (Gamma_i) for the strength calculation\n",
    "        self.Gamma = np.cumsum(self.gamma)\n",
    "        \n",
    "        # 2. Initialize the state of the integrators (x_i)\n",
    "        # All pools start empty (0.0) before the user has ever seen the word\n",
    "        self.x = np.zeros(N)\n",
    "\n",
    "    def decay(self, t):\n",
    "        \"\"\"Decays the memory state over time t (in days).\"\"\"\n",
    "        if t > 0:\n",
    "            self.x = self.x * np.exp(-t / self.tau)\n",
    "\n",
    "    def get_strengths(self):\n",
    "        \"\"\"Calculates the net strength (s_i) at each scale.\"\"\"\n",
    "        weighted_x = self.gamma * self.x\n",
    "        cum_weighted_x = np.cumsum(weighted_x)\n",
    "        return cum_weighted_x / self.Gamma\n",
    "\n",
    "    def predict(self, t):\n",
    "        \"\"\"Predicts the probability of recall after time t.\"\"\"\n",
    "        # Calculate what the state *will be* after time t\n",
    "        decayed_x = self.x * np.exp(-t / self.tau)\n",
    "        weighted_x = self.gamma * decayed_x\n",
    "        \n",
    "        # Global strength is the sum across all N pools\n",
    "        s_N = np.sum(weighted_x) / self.Gamma[-1] \n",
    "        return np.clip(s_N, 0.0001, 0.9999)\n",
    "\n",
    "    def study(self, t, recalled):\n",
    "        \"\"\"\n",
    "        Updates the memory state after a study attempt.\n",
    "        - t: Time elapsed since the LAST study session.\n",
    "        - recalled: Boolean or 1/0 indicating if the user got it right.\n",
    "        \"\"\"\n",
    "        # 1. Decay the memory by the time elapsed since last review\n",
    "        self.decay(t)\n",
    "        \n",
    "        # 2. Calculate current strength before learning\n",
    "        s = self.get_strengths()\n",
    "        \n",
    "        # 3. Determine learning rate based on retrieval success\n",
    "        # If they recalled it, the boost is eps_r (e.g., 9). If they forgot, it's 1.\n",
    "        eps = self.eps_r if recalled else 1.0\n",
    "        \n",
    "        # 4. Error-correction update: Pools only fill up if earlier pools failed to represent the item\n",
    "        delta_x = eps * (1.0 - s)\n",
    "        self.x = np.clip(self.x + delta_x, 0.0, 1.0)\n",
    "\n",
    "\n",
    "def generate_mcm_features(df):\n",
    "    \"\"\"\n",
    "    Highly optimized generator that bypasses Pandas iterrows overhead \n",
    "    by using NumPy arrays and tuple dictionary keys.\n",
    "    \"\"\"\n",
    "    print(\"Generating MCM predictions...\")\n",
    "    \n",
    "    # 1. Sort chronologically (Critical for time-series memory models)\n",
    "    df = df.sort_values(by=['user_id', 'lexeme_string', 'timestamp'])\n",
    "    \n",
    "    # 2. Pre-compute values so we don't do math inside the loop\n",
    "    t_days_array = (df['delta'] / (60 * 60 * 24)).to_numpy()\n",
    "    recalled_array = (df['session_correct'] > 0).to_numpy()\n",
    "    \n",
    "    # Extract to pure numpy arrays for ultra-fast iteration\n",
    "    users = df['user_id'].to_numpy()\n",
    "    words = df['lexeme_string'].to_numpy()\n",
    "    \n",
    "    # Pre-allocate output array for speed\n",
    "    mcm_predictions = np.zeros(len(df))\n",
    "    \n",
    "    # Dictionary to hold the state objects\n",
    "    user_item_states = {}\n",
    "    \n",
    "    for i in range(len(df)):\n",
    "        key = (users[i], words[i]) \n",
    "        \n",
    "        # Initialize MCM for this user-word pair if unseen\n",
    "        if key not in user_item_states:\n",
    "            user_item_states[key] = MultiscaleContextModel()\n",
    "            \n",
    "        mcm = user_item_states[key]\n",
    "        \n",
    "        # Predict probability right now\n",
    "        mcm_predictions[i] = mcm.predict(t_days_array[i])\n",
    "        \n",
    "        # Update the state based on the actual outcome\n",
    "        mcm.study(t_days_array[i], recalled_array[i])\n",
    "        \n",
    "    # 4. Re-attach to the dataframe\n",
    "    df['mcm_predicted_p'] = mcm_predictions\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77eec9af",
   "metadata": {},
   "source": [
    "## Feature engineering (we can definitely work on this)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7ee35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "VERB_TAGS = {'vblex', 'vaux', 'vbdo', 'vbhaver', 'vbmod', 'vbser'}\n",
    "PAST_TAGS = {'past', 'pii', 'ifi', 'pp'}\n",
    "PLURAL_TAGS = {'pl', 'sp'}\n",
    "NOUN_TAGS = {'n', 'np', 'prn', 'prpers'}\n",
    "GENDER_TAGS = {'m', 'f', 'nt', 'mf'}\n",
    "\n",
    "# 2. The Advanced Parser\n",
    "def parse_lexeme(lexeme_str):\n",
    "    try:\n",
    "        parts = str(lexeme_str).split('<')\n",
    "        \n",
    "        # Extract the lemma\n",
    "        word_info = parts[0].split('/')\n",
    "        lemma = word_info[1] if len(word_info) > 1 else word_info[0]\n",
    "        \n",
    "        # Extract tags and convert to a Python SET for instant math matching\n",
    "        tags = {t.replace('>', '') for t in parts[1:]}\n",
    "        \n",
    "        return {\n",
    "            'lemma': lemma,\n",
    "            'morph_complexity': len(tags) - 1 if len(tags) > 0 else 0,\n",
    "            \n",
    "            'is_verb': 1 if tags & VERB_TAGS else 0,\n",
    "            'is_noun': 1 if tags & NOUN_TAGS else 0,\n",
    "            'is_past_tense': 1 if tags & PAST_TAGS else 0,\n",
    "            'is_plural': 1 if tags & PLURAL_TAGS else 0,\n",
    "            'has_gender': 1 if tags & GENDER_TAGS else 0,\n",
    "            \n",
    "            # Keep the primary POS tag just in case\n",
    "            'base_pos': list(tags)[0] if tags else 'unknown'\n",
    "        }\n",
    "    except:\n",
    "        return {\n",
    "            'lemma': 'unknown', 'morph_complexity': 0, 'is_verb': 0, \n",
    "            'is_noun': 0, 'is_past_tense': 0, 'is_plural': 0, \n",
    "            'has_gender': 0, 'base_pos': 'unknown'\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ee0c18cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_xgboost_data(input_file, user_fraction=0.1):\n",
    "    print(f\"Reading data from {input_file}...\")\n",
    "    \n",
    "    # 1. Load Data\n",
    "    df = pd.read_csv(input_file, compression='infer')\n",
    "\n",
    "    parsed_list = [parse_lexeme(str(lex)) for lex in df['lexeme_string']]\n",
    "    \n",
    "    # Build the DataFrame once, in bulk\n",
    "    lexeme_features = pd.DataFrame(parsed_list, index=df.index)\n",
    "    \n",
    "    # Merge it back into the main dataframe\n",
    "    df = pd.concat([df, lexeme_features], axis=1)\n",
    "\n",
    "    original_len = len(df)\n",
    "    if user_fraction < 1.0:\n",
    "        print(f\"Shrinking dataset to {user_fraction*100}% of users for a fast trial...\")\n",
    "        unique_users = df['user_id'].unique()\n",
    "        \n",
    "        # Set a random seed so your trial is reproducible!\n",
    "        np.random.seed(42) \n",
    "        sampled_users = np.random.choice(\n",
    "            unique_users, \n",
    "            size=int(len(unique_users) * user_fraction), \n",
    "            replace=False\n",
    "        )\n",
    "        \n",
    "        # Filter the dataframe to only include our chosen users\n",
    "        df = df[df['user_id'].isin(sampled_users)].copy()\n",
    "        print(f\"Trial Data ready: Shrunk from {original_len} rows to {len(df)} rows.\")\n",
    "    \n",
    "    df = generate_mcm_features(df)\n",
    "    # cache_file = \"mcm_cached_dataset.pkl\"\n",
    "    \n",
    "    # # If we already ran MCM once, just load the saved file!\n",
    "    # if os.path.exists(cache_file) :\n",
    "    #     print(\"üü¢ Found cached MCM data! Loading instantly...\")\n",
    "    #     df = pd.read_pickle(cache_file)\n",
    "    # else:\n",
    "    #     print(f\"üü° No cache found. Reading raw data from {input_file}...\")\n",
    "\n",
    "    #     # Run the heavy 30-minute MCM math\n",
    "    #     df = generate_mcm_features(df)\n",
    "    #     print(\"üíæ Saving MCM data to cache...\")\n",
    "    #     df.to_pickle(cache_file)\n",
    "\n",
    "\n",
    "    df['pos_tag'] = df['lexeme_string'].str.extract(r'<([^>]+)>').fillna('unknown')\n",
    "    df['historical_accuracy'] = np.where(\n",
    "        df['history_seen'] > 0, \n",
    "        df['history_correct'] / df['history_seen'], \n",
    "        0.0\n",
    "    )\n",
    "    \n",
    "    # 5. Global User Accuracy\n",
    "    user_acc = df.groupby('user_id').apply(\n",
    "        lambda x: x['history_correct'].sum() / (x['history_seen'].sum() + 1e-5)\n",
    "    ).reset_index(name='user_global_accuracy')\n",
    "    df = df.merge(user_acc, on='user_id', how='left')\n",
    "    df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"], unit='s')\n",
    "\n",
    "    \n",
    "    # 2. Vectorized Target Variable (y)\n",
    "    y = df['p_recall'].clip(lower=0.0001, upper=0.9999)\n",
    "\n",
    "\n",
    "\n",
    "    # 3. Vectorized Feature Engineering (X)\n",
    "    X = pd.DataFrame()\n",
    "\n",
    "    #Time features\n",
    "    X['hour_of_day'] = df['timestamp'].dt.hour\n",
    "    X['day_of_week'] = df['timestamp'].dt.dayofweek\n",
    "\n",
    "    #Delta represents the time since last practice in days, which is critical for the forgetting curve\n",
    "    #DECIDE WHICH ONE TO KEEP: raw delta or log delta (log can help with skew but may lose interpretability)\n",
    "    X['time_lag_days'] = df['delta'] / (60 * 60 * 24)\n",
    "    #X['log_delta'] = np.log1p(df['delta'] / (60 * 60 * 24))\n",
    "\n",
    "    # 5. LEXEME COMPLEXITY \n",
    "    X['morph_complexity'] = df['morph_features']\n",
    "    X['is_verb'] = df['is_verb']\n",
    "    X['word_length'] = df['lemma'].str.len()\n",
    "\n",
    "\n",
    "    # Combine languages into a single feature\n",
    "    X['lang'] = df['ui_language'] + \"->\" + df['learning_language']\n",
    "\n",
    "    #MCM predictions model\n",
    "    X['mcm_predicted_p'] = df['mcm_predicted_p']\n",
    "\n",
    "\n",
    "    #Accuracy features\n",
    "    X['historical_accuracy'] = df['historical_accuracy']            # Micro: Their skill on this word\n",
    "    X['user_global_accuracy'] = df['user_global_accuracy']      # Macro: Their overall app skill\n",
    "    X['right'] = np.sqrt(1 + df['history_correct'])             #Raw success count (sqrt to reduce skew)\n",
    "    X['wrong'] = np.sqrt(1 + (df['history_seen'] - df['history_correct']))      #Raw failure count (sqrt authors mentioned it works better this way )\n",
    "                                       \n",
    "    # 4. The XGBoost Categorical Magic\n",
    "    X['lang'] = X['lang'].astype('category')\n",
    "    #X['lexeme_string'] = df['lexeme_string'].astype('category')\n",
    "    X['pos_tag'] = df['pos_tag'].astype('category')\n",
    "\n",
    "\n",
    "\n",
    "    print(\"Data processing complete! Splitting data...\")\n",
    "    \n",
    "    # 5. Fast 90/10 Split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82e9a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_xgboost_baseline(X_train, X_test, y_train, y_test):\n",
    "    print(\"Initializing XGBoost Regressor (Phase 2 Baseline)...\")\n",
    "    \n",
    "    # 1. Target Transformation: Calculate 'h' for training\n",
    "    # We use X_train['time_lag_days'] for 't'\n",
    "    h_train = -X_train['time_lag_days'] / np.log2(y_train)\n",
    "    h_train = np.clip(h_train, MIN_HALF_LIFE, MAX_HALF_LIFE)\n",
    "    \n",
    "    h_test = -X_test['time_lag_days'] / np.log2(y_test)\n",
    "    h_test = np.clip(h_test, MIN_HALF_LIFE, MAX_HALF_LIFE)\n",
    "    \n",
    "    # 1. The Model Architecture\n",
    "    # tree_method=\"hist\" is required for native categorical support\n",
    "    training_features = [col for col in X_train.columns ]\n",
    "    # We set a high n_estimators but use early_stopping to prevent overfitting\n",
    "    model = xgb.XGBRegressor(\n",
    "        tree_method=\"hist\", \n",
    "        enable_categorical=True,\n",
    "        n_estimators=10000,\n",
    "        learning_rate=0.01,\n",
    "        max_depth=6,\n",
    "        early_stopping_rounds=50,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    print(\"Training model... (Monitor validation loss to prevent overfitting)\")\n",
    "    \n",
    "    # 2. The Training Loop\n",
    "    # We pass the test set as the eval_set so the model can track its out-of-sample error\n",
    "    model.fit(\n",
    "        X_train[training_features], h_train,\n",
    "        eval_set=[(X_train[training_features], h_train), (X_test[training_features], h_test)],\n",
    "        verbose=50 # Prints update every 50 trees\n",
    "    )\n",
    "\n",
    "    # 2. Predict h\n",
    "    h_pred = model.predict(X_test[training_features])\n",
    "    h_pred = np.clip(h_pred, MIN_HALF_LIFE, MAX_HALF_LIFE)\n",
    "    \n",
    "    # 3. Transform h back to probability of recall (p)\n",
    "    p_pred = 2.0 ** (-X_test['time_lag_days'] / h_pred)\n",
    "    p_pred = np.clip(p_pred, 0.0001, 0.9999)\n",
    "\n",
    "    # 4. Evaluation Metrics\n",
    "    mae_h = mean_absolute_error(h_test, h_pred)\n",
    "    spearman_h, _ = spearmanr(h_test, h_pred)\n",
    "    \n",
    "    # --- Probability of Recall (p) Metrics ---\n",
    "    mae_p = mean_absolute_error(y_test, p_pred)\n",
    "    spearman_p,_ = spearmanr(y_test, p_pred)\n",
    "\n",
    "    print(\"-\" * 30)\n",
    "    print(\"üèÜ PHASE 2 RESULTS üèÜ\")\n",
    "    print(f\"  MAE (Days):           {mae_h:.4f}\")\n",
    "    print(f\"  Spearman Correlation: {spearman_h:.4f}\")\n",
    "    print(\"-\" * 40)\n",
    "    print(\"RECALL PROBABILITY (p) PREDICTION:\")\n",
    "    print(f\"  MAE:                  {mae_p:.4f}\")\n",
    "    print(f\"  Spearman Correlation: {spearman_p:.4f}\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46094c09",
   "metadata": {},
   "source": [
    "## We should try to perform some type of hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b26bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "import optuna\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "def optimize_xgboost_deep(X_train_full, y_train_full_p):\n",
    "    print(\"Initializing Deep Optuna Hyperparameter Hunt!\")\n",
    "    \n",
    "    X_train_opt, X_val_opt, y_train_opt_p, y_val_opt_p = train_test_split(\n",
    "        X_train_full, y_train_full_p, test_size=0.2, random_state=42\n",
    "    )\n",
    "    \n",
    "    # Safely clip to avoid log2(1.0) division by zero\n",
    "    safe_y_train = np.clip(y_train_opt_p, 0.001, 0.999)\n",
    "    safe_y_val = np.clip(y_val_opt_p, 0.001, 0.999)\n",
    "\n",
    "    # Convert p to h for training target\n",
    "    h_train_opt = np.clip(-X_train_opt['time_lag_days'] / np.log2(safe_y_train), MIN_HALF_LIFE, MAX_HALF_LIFE)\n",
    "    h_val_opt = np.clip(-X_val_opt['time_lag_days'] / np.log2(safe_y_val), MIN_HALF_LIFE, MAX_HALF_LIFE)\n",
    "\n",
    "    def objective(trial):\n",
    "        # 1. The Expanded Search Space\n",
    "        params = {\n",
    "            \"tree_method\": \"hist\",\n",
    "            \"enable_categorical\": True,\n",
    "            # Give it plenty of trees, early stopping will catch it when it's done\n",
    "            \"n_estimators\": 12000, \n",
    "            \n",
    "            # Allow for very slow, precise learning\n",
    "            \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.005, 0.1, log=True),\n",
    "            \n",
    "            # Tree architecture\n",
    "            \"max_depth\": trial.suggest_int(\"max_depth\", 3, 12),\n",
    "            \"min_child_weight\": trial.suggest_int(\"min_child_weight\", 1, 20),\n",
    "            \n",
    "            # Row and Column sampling (prevents overfitting)\n",
    "            \"subsample\": trial.suggest_float(\"subsample\", 0.5, 1.0),\n",
    "            \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.5, 1.0),\n",
    "            \n",
    "            # üõ°Ô∏è NEW: Regularization (Crucial for deep searches)\n",
    "            \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 1e-8, 10.0, log=True),   # L1 penalty\n",
    "            \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 1e-8, 10.0, log=True), # L2 penalty\n",
    "            \"gamma\": trial.suggest_float(\"gamma\", 1e-8, 5.0, log=True),            # Complexity control\n",
    "            \n",
    "            \"random_state\": 42,\n",
    "            \"early_stopping_rounds\": 50, # Wait longer before giving up\n",
    "            \"eval_metric\": \"rmse\"\n",
    "        }\n",
    "\n",
    "        model = xgb.XGBRegressor(**params)\n",
    "\n",
    "        model.fit(\n",
    "            X_train_opt, h_train_opt,\n",
    "            eval_set=[(X_val_opt, h_val_opt)],\n",
    "            verbose=False\n",
    "        )\n",
    "\n",
    "        # 3. Predict h and transform back to p_recall\n",
    "        h_pred = np.clip(model.predict(X_val_opt), MIN_HALF_LIFE, MAX_HALF_LIFE)\n",
    "        p_pred = np.clip(2.0 ** (-X_val_opt['time_lag_days'] / h_pred), 0.0001, 0.9999)\n",
    "\n",
    "        return mean_absolute_error(y_val_opt_p, p_pred)\n",
    "\n",
    "    # 2. Run the study (No aggressive pruning, let it explore fully)\n",
    "    study = optuna.create_study(direction=\"minimize\")\n",
    "    \n",
    "    # 100 trials is the sweet spot for a comprehensive XGBoost search\n",
    "    study.optimize(objective, n_trials=300) \n",
    "\n",
    "    print(\"\\nüèÜ DEEP OPTUNA SEARCH COMPLETE üèÜ\")\n",
    "    print(\"Best Trial MAE:\", study.best_value)\n",
    "    \n",
    "    return study.best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f3cb39",
   "metadata": {},
   "source": [
    "## Here we can focus on creating some cool visuals with SHAP (to see which parameters are most important) and PCA we should see"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca18c0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_pitch_deck_visuals(model, X_train, X_test):\n",
    "    print(\"Initializing SHAP TreeExplainer...\")\n",
    "    \n",
    "    # SHAP requires the underlying booster for XGBoost categorical data\n",
    "    explainer = shap.TreeExplainer(model)\n",
    "    \n",
    "    # Calculate SHAP values for a random sample of the test set \n",
    "    # (Using 10,000 rows to keep computation fast during the hackathon)\n",
    "    X_sample = X_test.sample(n=10000, random_state=42)\n",
    "    shap_values = explainer(X_sample)\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # GRAPH 1: The \"What Matters Most\" Slide (Summary Plot)\n",
    "    # ---------------------------------------------------------\n",
    "    print(\"Generating Feature Importance Graph...\")\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.title(\"What Drives Human Forgetting? (SHAP Feature Importance)\")\n",
    "    shap.summary_plot(shap_values, X_sample, show=False)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"slide1_feature_importance.png\", dpi=300)\n",
    "    plt.clf()\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # GRAPH 2: The \"When to Study\" Slide (Dependence Plot)\n",
    "    # ---------------------------------------------------------\n",
    "    print(\"Generating Circadian Rhythm Insight Graph...\")\n",
    "    # This shows how the hour of the day impacts the memory half-life\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.title(\"The Synchrony Effect: Time of Day vs. Memory Retention\")\n",
    "    shap.dependence_plot(\"hour_of_day\", shap_values.values, X_sample, show=False)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"slide2_circadian_rhythm.png\", dpi=300)\n",
    "    plt.clf()\n",
    "    \n",
    "    print(\"‚úÖ Visuals saved! Check your directory for the PNG files.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "721d97bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data from ../data/SpacedRepetitionData.csv...\n",
      "Shrinking dataset to 10.0% of users for a fast trial...\n",
      "Trial Data ready: Shrunk from 12854226 rows to 1300898 rows.\n",
      "Generating MCM predictions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\juanm\\AppData\\Local\\Temp\\ipykernel_46128\\4078707061.py:56: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  user_acc = df.groupby('user_id').apply(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data processing complete! Splitting data...\n",
      "Initializing XGBoost Regressor (Phase 2 Baseline)...\n",
      "Training model... (Monitor validation loss to prevent overfitting)\n",
      "[0]\tvalidation_0-rmse:123.14684\tvalidation_1-rmse:123.06647\n",
      "[50]\tvalidation_0-rmse:97.28371\tvalidation_1-rmse:97.06410\n",
      "[100]\tvalidation_0-rmse:85.84205\tvalidation_1-rmse:85.51313\n",
      "[150]\tvalidation_0-rmse:81.21247\tvalidation_1-rmse:80.81547\n",
      "[200]\tvalidation_0-rmse:79.41175\tvalidation_1-rmse:78.98001\n",
      "[250]\tvalidation_0-rmse:78.70897\tvalidation_1-rmse:78.26085\n",
      "[300]\tvalidation_0-rmse:78.42130\tvalidation_1-rmse:77.96989\n",
      "[350]\tvalidation_0-rmse:78.28983\tvalidation_1-rmse:77.84281\n",
      "[400]\tvalidation_0-rmse:78.22192\tvalidation_1-rmse:77.78181\n",
      "[450]\tvalidation_0-rmse:78.18113\tvalidation_1-rmse:77.74831\n",
      "[500]\tvalidation_0-rmse:78.15215\tvalidation_1-rmse:77.72796\n",
      "[550]\tvalidation_0-rmse:78.12751\tvalidation_1-rmse:77.71364\n",
      "[600]\tvalidation_0-rmse:78.10360\tvalidation_1-rmse:77.70202\n",
      "[650]\tvalidation_0-rmse:78.08045\tvalidation_1-rmse:77.69149\n",
      "[700]\tvalidation_0-rmse:78.05862\tvalidation_1-rmse:77.68298\n",
      "[750]\tvalidation_0-rmse:78.03919\tvalidation_1-rmse:77.67390\n",
      "[800]\tvalidation_0-rmse:78.02245\tvalidation_1-rmse:77.66859\n",
      "[850]\tvalidation_0-rmse:78.00734\tvalidation_1-rmse:77.66481\n",
      "[900]\tvalidation_0-rmse:77.99340\tvalidation_1-rmse:77.66185\n",
      "[950]\tvalidation_0-rmse:77.97897\tvalidation_1-rmse:77.65776\n",
      "[999]\tvalidation_0-rmse:77.96171\tvalidation_1-rmse:77.65244\n",
      "------------------------------\n",
      "üèÜ PHASE 2 RESULTS üèÜ\n",
      "  MAE (Days):           48.2495\n",
      "  Spearman Correlation: 0.6691\n",
      "----------------------------------------\n",
      "RECALL PROBABILITY (p) PREDICTION:\n",
      "  MAE:                  0.1153\n",
      "  Spearman Correlation: 0.0789\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = get_xgboost_data(\"../data/SpacedRepetitionData.csv\")\n",
    "baseline_model = train_xgboost_baseline(X_train, X_test, y_train, y_test)\n",
    "baseline_model.save_model(\"xgboost_baseline.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486b269e",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights=optimize_xgboost_deep(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d07b8701",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2=pd.read_csv(\"../data/SpacedRepetitionData.csv\", compression='infer')\n",
    "df2['pos_tag'] = df2['lexeme_string'].str.extract(r'<([^>]+)>').fillna('unknown')\n",
    "l=df2[\"pos_tag\"].value_counts()\n",
    "lines = [f\"{tag}: {count}\" for tag, count in l.items()]\n",
    "\n",
    "# 3. Write to file\n",
    "with open(\"pos_tags.txt\", \"w\") as f:\n",
    "    f.write(\"\\n\".join(lines))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "94c31a09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Extracting all unique tags from the dataset...\n",
      "\n",
      "‚úÖ Total unique tags found: 179\n",
      "\n",
      "üèÜ The Top 30 Most Common Tags:\n",
      "      Tag  Frequency\n",
      "0      sg    7548471\n",
      "1       n    5481000\n",
      "2       m    2345404\n",
      "3      pl    2251975\n",
      "4   vblex    2235300\n",
      "5     pri    2192234\n",
      "6       f    1842253\n",
      "7      p3    1702719\n",
      "8     det    1397539\n",
      "9     prn    1047261\n",
      "10     mf     996421\n",
      "11     p1     831120\n",
      "12    adj     711698\n",
      "13  vbser     547420\n",
      "14    nom     504196\n",
      "15    pos     485353\n",
      "16    def     473612\n",
      "17   pres     465160\n",
      "18    adv     453558\n",
      "19    ind     451196\n",
      "20    *sf     431410\n",
      "21  *numb     429807\n",
      "22     nt     424227\n",
      "23     sp     420619\n",
      "24     tn     414390\n",
      "25     p2     412062\n",
      "26     pr     398348\n",
      "27   subj     295716\n",
      "28    acc     293237\n",
      "29    itg     188256\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "# Assuming your dataframe is already loaded as 'df'\n",
    "print(\"üîç Extracting all unique tags from the dataset...\")\n",
    "\n",
    "# 1. Use Regex to find everything inside angle brackets < >\n",
    "# This is incredibly fast and grabs all tags at once\n",
    "all_tags = []\n",
    "for lexeme in df2['lexeme_string'].dropna():\n",
    "    # re.findall looks for anything between < and >\n",
    "    tags = re.findall(r'<([^>]+)>', str(lexeme))\n",
    "    all_tags.extend(tags)\n",
    "\n",
    "# 2. Count how many times each tag appears\n",
    "tag_counts = Counter(all_tags)\n",
    "\n",
    "# 3. Convert to a clean DataFrame so we can examine it easily\n",
    "tag_df = pd.DataFrame(tag_counts.items(), columns=['Tag', 'Frequency'])\n",
    "tag_df = tag_df.sort_values(by='Frequency', ascending=False).reset_index(drop=True)\n",
    "\n",
    "# 4. Show the results\n",
    "print(f\"\\n‚úÖ Total unique tags found: {len(tag_df)}\")\n",
    "print(\"\\nüèÜ The Top 30 Most Common Tags:\")\n",
    "print(tag_df.head(30))\n",
    "\n",
    "# Optional: Save the full list to a CSV to read later\n",
    "# tag_df.to_csv('all_morphological_tags.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "38f14a8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hour_of_day</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>time_lag_days</th>\n",
       "      <th>log_delta</th>\n",
       "      <th>morph_complexity</th>\n",
       "      <th>is_verb</th>\n",
       "      <th>word_length</th>\n",
       "      <th>lexeme_string</th>\n",
       "      <th>lang</th>\n",
       "      <th>mcm_predicted_p</th>\n",
       "      <th>historical_accuracy</th>\n",
       "      <th>user_global_accuracy</th>\n",
       "      <th>right</th>\n",
       "      <th>wrong</th>\n",
       "      <th>pos_tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>105800</th>\n",
       "      <td>20</td>\n",
       "      <td>6</td>\n",
       "      <td>1.763866</td>\n",
       "      <td>1.016630</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>horse/horse&lt;n&gt;&lt;sg&gt;</td>\n",
       "      <td>es-&gt;en</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.973059</td>\n",
       "      <td>0.938498</td>\n",
       "      <td>24.799194</td>\n",
       "      <td>4.242641</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1107017</th>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>0.019572</td>\n",
       "      <td>0.019383</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>girls/girl&lt;n&gt;&lt;pl&gt;</td>\n",
       "      <td>es-&gt;en</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.866487</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>551079</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.006505</td>\n",
       "      <td>0.006484</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>like/like&lt;vblex&gt;&lt;pres&gt;</td>\n",
       "      <td>es-&gt;en</td>\n",
       "      <td>0.655143</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.925083</td>\n",
       "      <td>3.872983</td>\n",
       "      <td>1.732051</td>\n",
       "      <td>vblex</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>812199</th>\n",
       "      <td>21</td>\n",
       "      <td>4</td>\n",
       "      <td>0.003067</td>\n",
       "      <td>0.003062</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>sono/essere&lt;vbser&gt;&lt;pri&gt;&lt;p3&gt;&lt;pl&gt;</td>\n",
       "      <td>en-&gt;it</td>\n",
       "      <td>0.816992</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.928069</td>\n",
       "      <td>3.741657</td>\n",
       "      <td>1.732051</td>\n",
       "      <td>vbser</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>545076</th>\n",
       "      <td>14</td>\n",
       "      <td>2</td>\n",
       "      <td>0.001563</td>\n",
       "      <td>0.001561</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>is/be&lt;vbser&gt;&lt;pri&gt;&lt;p3&gt;&lt;sg&gt;</td>\n",
       "      <td>pt-&gt;en</td>\n",
       "      <td>0.901620</td>\n",
       "      <td>0.924779</td>\n",
       "      <td>0.890789</td>\n",
       "      <td>14.491377</td>\n",
       "      <td>4.242641</td>\n",
       "      <td>vbser</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         hour_of_day  day_of_week  time_lag_days  log_delta  morph_complexity  \\\n",
       "105800            20            6       1.763866   1.016630                 1   \n",
       "1107017           15            1       0.019572   0.019383                 1   \n",
       "551079             0            1       0.006505   0.006484                 1   \n",
       "812199            21            4       0.003067   0.003062                 3   \n",
       "545076            14            2       0.001563   0.001561                 3   \n",
       "\n",
       "         is_verb  word_length                    lexeme_string    lang  \\\n",
       "105800         0            5               horse/horse<n><sg>  es->en   \n",
       "1107017        0            4                girls/girl<n><pl>  es->en   \n",
       "551079         1            4           like/like<vblex><pres>  es->en   \n",
       "812199         0            6  sono/essere<vbser><pri><p3><pl>  en->it   \n",
       "545076         0            2        is/be<vbser><pri><p3><sg>  pt->en   \n",
       "\n",
       "         mcm_predicted_p  historical_accuracy  user_global_accuracy  \\\n",
       "105800          0.000100             0.973059              0.938498   \n",
       "1107017         0.000100             1.000000              0.866487   \n",
       "551079          0.655143             0.875000              0.925083   \n",
       "812199          0.816992             0.866667              0.928069   \n",
       "545076          0.901620             0.924779              0.890789   \n",
       "\n",
       "             right     wrong pos_tag  \n",
       "105800   24.799194  4.242641       n  \n",
       "1107017   2.000000  1.000000       n  \n",
       "551079    3.872983  1.732051   vblex  \n",
       "812199    3.741657  1.732051   vbser  \n",
       "545076   14.491377  4.242641   vbser  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kulhack",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
