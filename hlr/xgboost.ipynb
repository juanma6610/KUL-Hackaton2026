{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1e39e3d",
   "metadata": {},
   "source": [
    "XGBoost implementation of the half life regression baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905252bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\juanm\\miniconda3\\envs\\kulhack\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from scipy.stats import spearmanr\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "from sys import intern\n",
    "import optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541427d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# various constraints on parameters and outputs\n",
    "MIN_HALF_LIFE = 15.0 / (24 * 60)    # 15 minutes\n",
    "MAX_HALF_LIFE = 274.                # 9 months\n",
    "LN2 = math.log(2.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b52af8fb",
   "metadata": {},
   "source": [
    "## MCM Portion of the tree (May need revisting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0fb9db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MultiscaleContextModel:\n",
    "    def __init__(self, mu=0.01, nu=1.05, xi=0.9, N=100, eps_r=9.0):\n",
    "        \"\"\"\n",
    "        Initializes the MCM memory state for a single item.\n",
    "        \n",
    "        Parameters:\n",
    "        - mu, nu: Control the distribution of time scales (decay rates).\n",
    "        - xi: Controls the weighting of different time scales.\n",
    "        - N: Number of context pools (integrators).\n",
    "        - eps_r: The boost given to successful retrieval (usually > 1).\n",
    "        \"\"\"\n",
    "        self.N = N\n",
    "        self.eps_r = eps_r\n",
    "        \n",
    "        # 1. Initialize Time Scales (tau) and Weights (gamma)\n",
    "        indices = np.arange(1, N + 1)\n",
    "        self.tau = mu * (nu ** indices)\n",
    "        self.gamma = xi ** indices\n",
    "        self.gamma = self.gamma / np.sum(self.gamma) # Normalize to sum to 1\n",
    "        \n",
    "        # Precompute cumulative sums of gamma (Gamma_i) for the strength calculation\n",
    "        self.Gamma = np.cumsum(self.gamma)\n",
    "        \n",
    "        # 2. Initialize the state of the integrators (x_i)\n",
    "        # All pools start empty (0.0) before the user has ever seen the word\n",
    "        self.x = np.zeros(N)\n",
    "\n",
    "    def decay(self, t):\n",
    "        \"\"\"Decays the memory state over time t (in days).\"\"\"\n",
    "        if t > 0:\n",
    "            self.x = self.x * np.exp(-t / self.tau)\n",
    "\n",
    "    def get_strengths(self):\n",
    "        \"\"\"Calculates the net strength (s_i) at each scale.\"\"\"\n",
    "        weighted_x = self.gamma * self.x\n",
    "        cum_weighted_x = np.cumsum(weighted_x)\n",
    "        return cum_weighted_x / self.Gamma\n",
    "\n",
    "    def predict(self, t):\n",
    "        \"\"\"Predicts the probability of recall after time t.\"\"\"\n",
    "        # Calculate what the state *will be* after time t\n",
    "        decayed_x = self.x * np.exp(-t / self.tau)\n",
    "        weighted_x = self.gamma * decayed_x\n",
    "        \n",
    "        # Global strength is the sum across all N pools\n",
    "        s_N = np.sum(weighted_x) / self.Gamma[-1] \n",
    "        return np.clip(s_N, 0.0001, 0.9999)\n",
    "\n",
    "    def study(self, t, recalled):\n",
    "        \"\"\"\n",
    "        Updates the memory state after a study attempt.\n",
    "        - t: Time elapsed since the LAST study session.\n",
    "        - recalled: Boolean or 1/0 indicating if the user got it right.\n",
    "        \"\"\"\n",
    "        # 1. Decay the memory by the time elapsed since last review\n",
    "        self.decay(t)\n",
    "        \n",
    "        # 2. Calculate current strength before learning\n",
    "        s = self.get_strengths()\n",
    "        \n",
    "        # 3. Determine learning rate based on retrieval success\n",
    "        # If they recalled it, the boost is eps_r (e.g., 9). If they forgot, it's 1.\n",
    "        eps = self.eps_r if recalled else 1.0\n",
    "        \n",
    "        # 4. Error-correction update: Pools only fill up if earlier pools failed to represent the item\n",
    "        delta_x = eps * (1.0 - s)\n",
    "        self.x = np.clip(self.x + delta_x, 0.0, 1.0)\n",
    "\n",
    "\n",
    "def generate_mcm_features(df):\n",
    "    \"\"\"\n",
    "    Highly optimized generator that bypasses Pandas iterrows overhead \n",
    "    by using NumPy arrays and tuple dictionary keys.\n",
    "    \"\"\"\n",
    "    print(\"Generating MCM predictions...\")\n",
    "    \n",
    "    # 1. Sort chronologically (Critical for time-series memory models)\n",
    "    df = df.sort_values(by=['user_id', 'lexeme_string', 'timestamp'])\n",
    "    \n",
    "    # 2. Pre-compute values so we don't do math inside the loop\n",
    "    t_days_array = (df['delta'] / (60 * 60 * 24)).to_numpy()\n",
    "    recalled_array = (df['session_correct'] > 0).to_numpy()\n",
    "    \n",
    "    # Extract to pure numpy arrays for ultra-fast iteration\n",
    "    users = df['user_id'].to_numpy()\n",
    "    words = df['lexeme_string'].to_numpy()\n",
    "    \n",
    "    # Pre-allocate output array for speed\n",
    "    mcm_predictions = np.zeros(len(df))\n",
    "    \n",
    "    # Dictionary to hold the state objects\n",
    "    user_item_states = {}\n",
    "    \n",
    "    for i in range(len(df)):\n",
    "        key = (users[i], words[i]) \n",
    "        \n",
    "        # Initialize MCM for this user-word pair if unseen\n",
    "        if key not in user_item_states:\n",
    "            user_item_states[key] = MultiscaleContextModel()\n",
    "            \n",
    "        mcm = user_item_states[key]\n",
    "        \n",
    "        # Predict probability right now\n",
    "        mcm_predictions[i] = mcm.predict(t_days_array[i])\n",
    "        \n",
    "        # Update the state based on the actual outcome\n",
    "        mcm.study(t_days_array[i], recalled_array[i])\n",
    "        \n",
    "    # 4. Re-attach to the dataframe\n",
    "    df['mcm_predicted_p'] = mcm_predictions\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77eec9af",
   "metadata": {},
   "source": [
    "## Feature engineering (we can definitely work on this)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0c18cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_xgboost_data(input_file, user_fraction=1):\n",
    "    print(f\"Reading data from {input_file}...\")\n",
    "    \n",
    "    # 1. Load Data\n",
    "    df = pd.read_csv(input_file, compression='infer')\n",
    "    original_len = len(df)\n",
    "    if user_fraction < 1.0:\n",
    "        print(f\"Shrinking dataset to {user_fraction*100}% of users for a fast trial...\")\n",
    "        unique_users = df['user_id'].unique()\n",
    "        \n",
    "        # Set a random seed so your trial is reproducible!\n",
    "        np.random.seed(42) \n",
    "        sampled_users = np.random.choice(\n",
    "            unique_users, \n",
    "            size=int(len(unique_users) * user_fraction), \n",
    "            replace=False\n",
    "        )\n",
    "        \n",
    "        # Filter the dataframe to only include our chosen users\n",
    "        df = df[df['user_id'].isin(sampled_users)].copy()\n",
    "        print(f\"Trial Data ready: Shrunk from {original_len} rows to {len(df)} rows.\")\n",
    "    \n",
    "    df = generate_mcm_features(df)\n",
    "    # cache_file = \"mcm_cached_dataset.pkl\"\n",
    "    \n",
    "    # # If we already ran MCM once, just load the saved file!\n",
    "    # if os.path.exists(cache_file) :\n",
    "    #     print(\"üü¢ Found cached MCM data! Loading instantly...\")\n",
    "    #     df = pd.read_pickle(cache_file)\n",
    "    # else:\n",
    "    #     print(f\"üü° No cache found. Reading raw data from {input_file}...\")\n",
    "\n",
    "    #     # Run the heavy 30-minute MCM math\n",
    "    #     df = generate_mcm_features(df)\n",
    "    #     print(\"üíæ Saving MCM data to cache...\")\n",
    "    #     df.to_pickle(cache_file)\n",
    "\n",
    "\n",
    "    df['pos_tag'] = df['lexeme_string'].str.extract(r'<([^>]+)>').fillna('unknown')\n",
    "    df['historical_accuracy'] = np.where(\n",
    "        df['history_seen'] > 0, \n",
    "        df['history_correct'] / df['history_seen'], \n",
    "        0.0\n",
    "    )\n",
    "    \n",
    "    # 5. Global User Accuracy\n",
    "    user_acc = df.groupby('user_id').apply(\n",
    "        lambda x: x['history_correct'].sum() / (x['history_seen'].sum() + 1e-5)\n",
    "    ).reset_index(name='user_global_accuracy')\n",
    "    df = df.merge(user_acc, on='user_id', how='left')\n",
    "    df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"], unit='s')\n",
    "\n",
    "    \n",
    "    # 2. Vectorized Target Variable (y)\n",
    "    y = df['p_recall'].clip(lower=0.0001, upper=0.9999)\n",
    "\n",
    "\n",
    "\n",
    "    # 3. Vectorized Feature Engineering (X)\n",
    "    X = pd.DataFrame()\n",
    "\n",
    "    #Time features\n",
    "    X['hour_of_day'] = df['timestamp'].dt.hour\n",
    "    X['day_of_week'] = df['timestamp'].dt.dayofweek\n",
    "\n",
    "    #Delta represents the time since last practice in days, which is critical for the forgetting curve\n",
    "    #DECIDE WHICH ONE TO KEEP: raw delta or log delta (log can help with skew but may lose interpretability)\n",
    "    X['time_lag_days'] = df['delta'] / (60 * 60 * 24)\n",
    "    X['log_delta'] = np.log1p(df['delta'] / (60 * 60 * 24))\n",
    "\n",
    "    # Combine languages into a single feature\n",
    "    X['lang'] = df['ui_language'] + \"->\" + df['learning_language']\n",
    "\n",
    "    #MCM predictions model\n",
    "    X['mcm_predicted_p'] = df['mcm_predicted_p']\n",
    "\n",
    "\n",
    "    #Accuracy features\n",
    "    X['historical_accuracy'] = df['historical_accuracy']            # Micro: Their skill on this word\n",
    "    X['user_global_accuracy'] = df['user_global_accuracy']      # Macro: Their overall app skill\n",
    "    X['right'] = np.sqrt(1 + df['history_correct'])             #Raw success count (sqrt to reduce skew)\n",
    "    X['wrong'] = np.sqrt(1 + (df['history_seen'] - df['history_correct']))      #Raw failure count (sqrt authors mentioned it works better this way )\n",
    "                                       \n",
    "    # 4. The XGBoost Categorical Magic\n",
    "    X['lang'] = X['lang'].astype('category')\n",
    "    #X['lexeme_string'] = df['lexeme_string'].astype('category')\n",
    "    X['pos_tag'] = df['pos_tag'].astype('category')\n",
    "\n",
    "\n",
    "\n",
    "    print(\"Data processing complete! Splitting data...\")\n",
    "    \n",
    "    # 5. Fast 90/10 Split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82e9a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_xgboost_baseline(X_train, X_test, y_train, y_test):\n",
    "    print(\"Initializing XGBoost Regressor (Phase 2 Baseline)...\")\n",
    "    \n",
    "    # 1. Target Transformation: Calculate 'h' for training\n",
    "    # We use X_train['time_lag_days'] for 't'\n",
    "    h_train = -X_train['time_lag_days'] / np.log2(y_train)\n",
    "    h_train = np.clip(h_train, MIN_HALF_LIFE, MAX_HALF_LIFE)\n",
    "    \n",
    "    h_test = -X_test['time_lag_days'] / np.log2(y_test)\n",
    "    h_test = np.clip(h_test, MIN_HALF_LIFE, MAX_HALF_LIFE)\n",
    "    \n",
    "    # 1. The Model Architecture\n",
    "    # tree_method=\"hist\" is required for native categorical support\n",
    "    training_features = [col for col in X_train.columns ]\n",
    "    # We set a high n_estimators but use early_stopping to prevent overfitting\n",
    "    model = xgb.XGBRegressor(\n",
    "        tree_method=\"hist\", \n",
    "        enable_categorical=True,\n",
    "        n_estimators=1000,\n",
    "        learning_rate=0.01,\n",
    "        max_depth=6,\n",
    "        early_stopping_rounds=50,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    print(\"Training model... (Monitor validation loss to prevent overfitting)\")\n",
    "    \n",
    "    # 2. The Training Loop\n",
    "    # We pass the test set as the eval_set so the model can track its out-of-sample error\n",
    "    model.fit(\n",
    "        X_train[training_features], h_train,\n",
    "        eval_set=[(X_train[training_features], h_train), (X_test[training_features], h_test)],\n",
    "        verbose=50 # Prints update every 50 trees\n",
    "    )\n",
    "\n",
    "    # 2. Predict h\n",
    "    h_pred = model.predict(X_test[training_features])\n",
    "    h_pred = np.clip(h_pred, MIN_HALF_LIFE, MAX_HALF_LIFE)\n",
    "    \n",
    "    # 3. Transform h back to probability of recall (p)\n",
    "    p_pred = 2.0 ** (-X_test['time_lag_days'] / h_pred)\n",
    "    p_pred = np.clip(p_pred, 0.0001, 0.9999)\n",
    "\n",
    "    # 4. Evaluation Metrics\n",
    "    mae_h = mean_absolute_error(h_test, h_pred)\n",
    "    spearman_h, _ = spearmanr(h_test, h_pred)\n",
    "    \n",
    "    # --- Probability of Recall (p) Metrics ---\n",
    "    mae_p = mean_absolute_error(y_test, p_pred)\n",
    "    spearman_p,_ = spearmanr(y_test, p_pred)\n",
    "\n",
    "    print(\"-\" * 30)\n",
    "    print(\"üèÜ PHASE 2 RESULTS üèÜ\")\n",
    "    print(f\"  MAE (Days):           {mae_h:.4f}\")\n",
    "    print(f\"  Spearman Correlation: {spearman_h:.4f}\")\n",
    "    print(\"-\" * 40)\n",
    "    print(\"RECALL PROBABILITY (p) PREDICTION:\")\n",
    "    print(f\"  MAE:                  {mae_p:.4f}\")\n",
    "    print(f\"  Spearman Correlation: {spearman_p:.4f}\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46094c09",
   "metadata": {},
   "source": [
    "## We should try to perform some type of hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b26bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_xgboost(X_train_full, y_train_full_p):\n",
    "    print(\"Initializing Optuna Hyperparameter Hunt...\")\n",
    "    \n",
    "    # Split training data to create a dedicated validation set for Optuna\n",
    "    # This prevents us from overfitting to our final holdout test set!\n",
    "    X_train_opt, X_val_opt, y_train_opt_p, y_val_opt_p = train_test_split(\n",
    "        X_train_full, y_train_full_p, test_size=0.2, random_state=42\n",
    "    )\n",
    "    \n",
    "    h_train_opt = np.clip(-X_train_opt['time_lag_days'] / np.log2(y_train_opt_p), MIN_HALF_LIFE, MAX_HALF_LIFE)\n",
    "    h_val_opt = np.clip(-X_val_opt['time_lag_days'] / np.log2(y_val_opt_p), MIN_HALF_LIFE, MAX_HALF_LIFE)\n",
    "\n",
    "    def objective(trial):\n",
    "        # 1. Define the search space\n",
    "        params = {\n",
    "            \"tree_method\": \"hist\",\n",
    "            \"enable_categorical\": True,\n",
    "            \"n_estimators\": 500, # Keep lower for faster hackathon iteration\n",
    "            \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.001, 0.2, log=True),\n",
    "            \"max_depth\": trial.suggest_int(\"max_depth\", 4, 10),\n",
    "            \"min_child_weight\": trial.suggest_int(\"min_child_weight\", 1, 10),\n",
    "            \"subsample\": trial.suggest_float(\"subsample\", 0.6, 1.0),\n",
    "            \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.6, 1.0),\n",
    "            \"random_state\": 42\n",
    "        }\n",
    "\n",
    "        # 2. Train the model\n",
    "        model = xgb.XGBRegressor(**params)\n",
    "        model.fit(\n",
    "            X_train_opt, h_train_opt,\n",
    "            eval_set=[(X_val_opt, h_val_opt)],\n",
    "            verbose=False # Keep terminal clean\n",
    "        )\n",
    "\n",
    "        # 3. Predict h and transform back to p_recall\n",
    "        h_pred = np.clip(model.predict(X_val_opt), MIN_HALF_LIFE, MAX_HALF_LIFE)\n",
    "        p_pred = np.clip(2.0 ** (-X_val_opt['time_lag_days'] / h_pred), 0.0001, 0.9999)\n",
    "\n",
    "        # 4. Return the metric we want to minimize\n",
    "        return mean_absolute_error(y_val_opt_p, p_pred)\n",
    "\n",
    "    # Run the study\n",
    "    study = optuna.create_study(direction=\"minimize\")\n",
    "    study.optimize(objective, n_trials=30) # 30 trials is a good hackathon sweet spot\n",
    "\n",
    "    print(\"\\nüèÜ OPTUNA SEARCH COMPLETE üèÜ\")\n",
    "    print(\"Best Trial MAE:\", study.best_value)\n",
    "    print(\"Best Parameters:\", study.best_params)\n",
    "    \n",
    "    return study.best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f3cb39",
   "metadata": {},
   "source": [
    "## Here we can focus on creating some cool visuals with SHAP (to see which parameters are most important) and PCA we should see"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca18c0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_pitch_deck_visuals(model, X_train, X_test):\n",
    "    print(\"Initializing SHAP TreeExplainer...\")\n",
    "    \n",
    "    # SHAP requires the underlying booster for XGBoost categorical data\n",
    "    explainer = shap.TreeExplainer(model)\n",
    "    \n",
    "    # Calculate SHAP values for a random sample of the test set \n",
    "    # (Using 10,000 rows to keep computation fast during the hackathon)\n",
    "    X_sample = X_test.sample(n=10000, random_state=42)\n",
    "    shap_values = explainer(X_sample)\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # GRAPH 1: The \"What Matters Most\" Slide (Summary Plot)\n",
    "    # ---------------------------------------------------------\n",
    "    print(\"Generating Feature Importance Graph...\")\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.title(\"What Drives Human Forgetting? (SHAP Feature Importance)\")\n",
    "    shap.summary_plot(shap_values, X_sample, show=False)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"slide1_feature_importance.png\", dpi=300)\n",
    "    plt.clf()\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # GRAPH 2: The \"When to Study\" Slide (Dependence Plot)\n",
    "    # ---------------------------------------------------------\n",
    "    print(\"Generating Circadian Rhythm Insight Graph...\")\n",
    "    # This shows how the hour of the day impacts the memory half-life\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.title(\"The Synchrony Effect: Time of Day vs. Memory Retention\")\n",
    "    shap.dependence_plot(\"hour_of_day\", shap_values.values, X_sample, show=False)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"slide2_circadian_rhythm.png\", dpi=300)\n",
    "    plt.clf()\n",
    "    \n",
    "    print(\"‚úÖ Visuals saved! Check your directory for the PNG files.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "721d97bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data from ../data/SpacedRepetitionData.csv...\n",
      "Data processing complete! Splitting data...\n",
      "Initializing XGBoost Regressor (Phase 2 Baseline)...\n",
      "Training model... (Monitor validation loss to prevent overfitting)\n",
      "[0]\tvalidation_0-rmse:116.83664\tvalidation_1-rmse:116.82482\n",
      "[50]\tvalidation_0-rmse:79.05515\tvalidation_1-rmse:79.00379\n",
      "[100]\tvalidation_0-rmse:79.00607\tvalidation_1-rmse:78.96547\n",
      "[150]\tvalidation_0-rmse:78.97959\tvalidation_1-rmse:78.95420\n",
      "[200]\tvalidation_0-rmse:78.96053\tvalidation_1-rmse:78.94784\n",
      "[250]\tvalidation_0-rmse:78.94333\tvalidation_1-rmse:78.94356\n",
      "[300]\tvalidation_0-rmse:78.92935\tvalidation_1-rmse:78.94199\n",
      "[350]\tvalidation_0-rmse:78.91467\tvalidation_1-rmse:78.94114\n",
      "[400]\tvalidation_0-rmse:78.90098\tvalidation_1-rmse:78.93942\n",
      "[450]\tvalidation_0-rmse:78.88664\tvalidation_1-rmse:78.93801\n",
      "[500]\tvalidation_0-rmse:78.87450\tvalidation_1-rmse:78.93647\n",
      "[550]\tvalidation_0-rmse:78.86218\tvalidation_1-rmse:78.93630\n",
      "[564]\tvalidation_0-rmse:78.85936\tvalidation_1-rmse:78.93659\n",
      "------------------------------\n",
      "üèÜ PHASE 2 RESULTS üèÜ\n",
      "  MAE (Days):           49.4702\n",
      "  Spearman Correlation: 0.6518\n",
      "----------------------------------------\n",
      "RECALL PROBABILITY (p) PREDICTION:\n",
      "  MAE:                  0.1178\n",
      "  Spearman Correlation: 0.0708\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = get_xgboost_data(\"../data/SpacedRepetitionData.csv\")\n",
    "baseline_model = train_xgboost_baseline(X_train, X_test, y_train, y_test)\n",
    "baseline_model.save_model(\"xgboost_baseline.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d07b8701",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_model = train_xgboost_baseline(X_train, X_test, y_train, y_test)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kulhack",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
