{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1e39e3d",
   "metadata": {},
   "source": [
    "XGBoost implementation of the half life regression baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "905252bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\juanm\\miniconda3\\envs\\kulhack\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from scipy.stats import spearmanr\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "from sys import intern\n",
    "import optuna\n",
    "\n",
    "from collections import defaultdict, namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "541427d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# various constraints on parameters and outputs\n",
    "from numpy.ma import mean\n",
    "\n",
    "\n",
    "MIN_HALF_LIFE = 15.0 / (24 * 60)    # 15 minutes\n",
    "MAX_HALF_LIFE = 274.                # 9 months\n",
    "LN2 = math.log(2.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee0c18cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_xgboost_data(input_file, max_lines=None):\n",
    "    print(f\"Reading data from {input_file}...\")\n",
    "    \n",
    "    # 1. Load Data\n",
    "    if max_lines is not None:\n",
    "        df = pd.read_csv(input_file, compression='infer', nrows=max_lines)\n",
    "    else:\n",
    "        df = pd.read_csv(input_file, compression='infer')\n",
    "\n",
    "    df['pos_tag'] = df['lexeme_string'].str.extract(r'<([^>]+)>').fillna('unknown')\n",
    "    df['historical_accuracy'] = np.where(\n",
    "        df['history_seen'] > 0, \n",
    "        df['history_correct'] / df['history_seen'], \n",
    "        0.0\n",
    "    )\n",
    "    \n",
    "    # 5. Global User Accuracy\n",
    "    user_acc = df.groupby('user_id').apply(\n",
    "        lambda x: x['history_correct'].sum() / (x['history_seen'].sum() + 1e-5)\n",
    "    ).reset_index(name='user_global_accuracy')\n",
    "    df = df.merge(user_acc, on='user_id', how='left')\n",
    "    df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"], unit='s')\n",
    "    \n",
    "    # 2. Vectorized Target Variable (y)\n",
    "    # Replaces the pclip() function by clipping the entire column at once\n",
    "    y = df['p_recall'].clip(lower=0.0001, upper=0.9999)\n",
    "\n",
    "    # 3. Vectorized Feature Engineering (X)\n",
    "    X = pd.DataFrame()\n",
    "\n",
    "\n",
    "    X['hour_of_day'] = df['timestamp'].dt.hour\n",
    "    X['day_of_week'] = df['timestamp'].dt.dayofweek\n",
    "\n",
    "    X['time_lag_days'] = df['delta'] / (60 * 60 * 24)\n",
    "    X['right'] = np.sqrt(1 + df['history_correct'])\n",
    "    X['wrong'] = np.sqrt(1 + (df['history_seen'] - df['history_correct']))\n",
    "    df['log_delta'] = np.log1p(df['delta'] / (60 * 60 * 24))\n",
    "    # Combine languages into a single feature\n",
    "    X['lang'] = df['ui_language'] + \"->\" + df['learning_language']\n",
    "\n",
    "    X['historical_accuracy'] = df['historical_accuracy']\n",
    "    X['log_delta'] = df['log_delta']\n",
    "    \n",
    "    # 4. The XGBoost Categorical Magic\n",
    "    # Cast strings to pandas categorical dtype. XGBoost will handle the rest!\n",
    "    X['lang'] = X['lang'].astype('category')\n",
    "    #X['lexeme_string'] = df['lexeme_string'].astype('category')\n",
    "    X['pos_tag'] = df['pos_tag'].astype('category')\n",
    "\n",
    "    print(\"Data processing complete! Splitting data...\")\n",
    "    \n",
    "    # 5. Fast 90/10 Split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82e9a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_xgboost_baseline(X_train, X_test, y_train, y_test):\n",
    "    print(\"Initializing XGBoost Regressor (Phase 2 Baseline)...\")\n",
    "    \n",
    "    # 1. Target Transformation: Calculate 'h' for training\n",
    "    # We use X_train['time_lag_days'] for 't'\n",
    "    h_train = -X_train['time_lag_days'] / np.log2(y_train)\n",
    "    h_train = np.clip(h_train, MIN_HALF_LIFE, MAX_HALF_LIFE)\n",
    "    \n",
    "    h_test = -X_test['time_lag_days'] / np.log2(y_test)\n",
    "    h_test = np.clip(h_test, MIN_HALF_LIFE, MAX_HALF_LIFE)\n",
    "    \n",
    "    # 1. The Model Architecture\n",
    "    # tree_method=\"hist\" is required for native categorical support\n",
    "    # We set a high n_estimators but use early_stopping to prevent overfitting\n",
    "    model = xgb.XGBRegressor(\n",
    "        tree_method=\"hist\", \n",
    "        enable_categorical=True,\n",
    "        n_estimators=1000,\n",
    "        learning_rate=0.001,\n",
    "        max_depth=6,\n",
    "        early_stopping_rounds=50,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    print(\"Training model... (Monitor validation loss to prevent overfitting)\")\n",
    "    \n",
    "    # 2. The Training Loop\n",
    "    # We pass the test set as the eval_set so the model can track its out-of-sample error\n",
    "    model.fit(\n",
    "        X_train, h_train,\n",
    "        eval_set=[(X_train, h_train), (X_test, h_test)],\n",
    "        verbose=50 # Prints update every 50 trees\n",
    "    )\n",
    "\n",
    "    # 2. Predict h\n",
    "    h_pred = model.predict(X_test)\n",
    "    h_pred = np.clip(h_pred, MIN_HALF_LIFE, MAX_HALF_LIFE)\n",
    "    \n",
    "    # 3. Transform h back to probability of recall (p)\n",
    "    p_pred = 2.0 ** (-X_test['time_lag_days'] / h_pred)\n",
    "    p_pred = np.clip(p_pred, 0.0001, 0.9999)\n",
    "\n",
    "    # 4. Evaluate against the ORIGINAL p_recall \n",
    "    mae_h = mean_absolute_error(h_test, h_pred)\n",
    "    spearman_h, _ = spearmanr(h_test, h_pred)\n",
    "    \n",
    "    # --- Probability of Recall (p) Metrics ---\n",
    "    mae_p = mean_absolute_error(y_test, p_pred)\n",
    "    spearman_p,_ = spearmanr(y_test, p_pred)\n",
    "\n",
    "    print(\"-\" * 30)\n",
    "    print(\"üèÜ PHASE 2 RESULTS üèÜ\")\n",
    "    print(f\"  MAE (Days):           {mae_h:.4f}\")\n",
    "    print(f\"  Spearman Correlation: {spearman_h:.4f}\")\n",
    "    print(\"-\" * 40)\n",
    "    print(\"RECALL PROBABILITY (p) PREDICTION:\")\n",
    "    print(f\"  MAE:                  {mae_p:.4f}\")\n",
    "    print(f\"  Spearman Correlation: {spearman_p:.4f}\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b26bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_xgboost(X_train_full, y_train_full_p):\n",
    "    print(\"Initializing Optuna Hyperparameter Hunt...\")\n",
    "    \n",
    "    # Split training data to create a dedicated validation set for Optuna\n",
    "    # This prevents us from overfitting to our final holdout test set!\n",
    "    X_train_opt, X_val_opt, y_train_opt_p, y_val_opt_p = train_test_split(\n",
    "        X_train_full, y_train_full_p, test_size=0.2, random_state=42\n",
    "    )\n",
    "    \n",
    "    h_train_opt = np.clip(-X_train_opt['time_lag_days'] / np.log2(y_train_opt_p), MIN_HALF_LIFE, MAX_HALF_LIFE)\n",
    "    h_val_opt = np.clip(-X_val_opt['time_lag_days'] / np.log2(y_val_opt_p), MIN_HALF_LIFE, MAX_HALF_LIFE)\n",
    "\n",
    "    def objective(trial):\n",
    "        # 1. Define the search space\n",
    "        params = {\n",
    "            \"tree_method\": \"hist\",\n",
    "            \"enable_categorical\": True,\n",
    "            \"n_estimators\": 500, # Keep lower for faster hackathon iteration\n",
    "            \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.001, 0.2, log=True),\n",
    "            \"max_depth\": trial.suggest_int(\"max_depth\", 4, 10),\n",
    "            \"min_child_weight\": trial.suggest_int(\"min_child_weight\", 1, 10),\n",
    "            \"subsample\": trial.suggest_float(\"subsample\", 0.6, 1.0),\n",
    "            \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.6, 1.0),\n",
    "            \"random_state\": 42\n",
    "        }\n",
    "\n",
    "        # 2. Train the model\n",
    "        model = xgb.XGBRegressor(**params)\n",
    "        model.fit(\n",
    "            X_train_opt, h_train_opt,\n",
    "            eval_set=[(X_val_opt, h_val_opt)],\n",
    "            verbose=False # Keep terminal clean\n",
    "        )\n",
    "\n",
    "        # 3. Predict h and transform back to p_recall\n",
    "        h_pred = np.clip(model.predict(X_val_opt), MIN_HALF_LIFE, MAX_HALF_LIFE)\n",
    "        p_pred = np.clip(2.0 ** (-X_val_opt['time_lag_days'] / h_pred), 0.0001, 0.9999)\n",
    "\n",
    "        # 4. Return the metric we want to minimize\n",
    "        return mean_absolute_error(y_val_opt_p, p_pred)\n",
    "\n",
    "    # Run the study\n",
    "    study = optuna.create_study(direction=\"minimize\")\n",
    "    study.optimize(objective, n_trials=30) # 30 trials is a good hackathon sweet spot\n",
    "\n",
    "    print(\"\\nüèÜ OPTUNA SEARCH COMPLETE üèÜ\")\n",
    "    print(\"Best Trial MAE:\", study.best_value)\n",
    "    print(\"Best Parameters:\", study.best_params)\n",
    "    \n",
    "    return study.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "721d97bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data from ../data/SpacedRepetitionData.csv...\n",
      "Data processing complete! Splitting data...\n",
      "Initializing XGBoost Regressor (Phase 2 Baseline)...\n",
      "Training model... (Monitor validation loss to prevent overfitting)\n",
      "[0]\tvalidation_0-rmse:116.83664\tvalidation_1-rmse:116.82482\n",
      "[50]\tvalidation_0-rmse:79.05515\tvalidation_1-rmse:79.00379\n",
      "[100]\tvalidation_0-rmse:79.00607\tvalidation_1-rmse:78.96547\n",
      "[150]\tvalidation_0-rmse:78.97959\tvalidation_1-rmse:78.95420\n",
      "[200]\tvalidation_0-rmse:78.96053\tvalidation_1-rmse:78.94784\n",
      "[250]\tvalidation_0-rmse:78.94333\tvalidation_1-rmse:78.94356\n",
      "[300]\tvalidation_0-rmse:78.92935\tvalidation_1-rmse:78.94199\n",
      "[350]\tvalidation_0-rmse:78.91467\tvalidation_1-rmse:78.94114\n",
      "[400]\tvalidation_0-rmse:78.90098\tvalidation_1-rmse:78.93942\n",
      "[450]\tvalidation_0-rmse:78.88664\tvalidation_1-rmse:78.93801\n",
      "[500]\tvalidation_0-rmse:78.87450\tvalidation_1-rmse:78.93647\n",
      "[550]\tvalidation_0-rmse:78.86218\tvalidation_1-rmse:78.93630\n",
      "[564]\tvalidation_0-rmse:78.85936\tvalidation_1-rmse:78.93659\n",
      "------------------------------\n",
      "üèÜ PHASE 2 RESULTS üèÜ\n",
      "  MAE (Days):           49.4702\n",
      "  Spearman Correlation: 0.6518\n",
      "----------------------------------------\n",
      "RECALL PROBABILITY (p) PREDICTION:\n",
      "  MAE:                  0.1178\n",
      "  Spearman Correlation: 0.0708\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = get_xgboost_data(\"../data/SpacedRepetitionData.csv\")\n",
    "baseline_model = train_xgboost_baseline(X_train, X_test, y_train, y_test)\n",
    "baseline_model.save_model(\"xgboost_baseline.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d07b8701",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_model = train_xgboost_baseline(X_train, X_test, y_train, y_test)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kulhack",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
